{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape Reddit Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credentials.\n",
    "creds = json.load(open('../Datasets/credentials.json'))\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                    client_secret=creds['client_secret'],\n",
    "                    user_agent=creds['user_agent'])\n",
    "subreddit = reddit.subreddit(\"jokes\")\n",
    "posts = []\n",
    "\n",
    "# Scrape top and hot subreddit sections.\n",
    "for submission in subreddit.top(limit=None):\n",
    "    text = submission.title + ' ' + submission.selftext\n",
    "    posts.append(text)\n",
    "    \n",
    "for submission in subreddit.hot(limit=None):\n",
    "    text = submission.title + ' ' + submission.selftext\n",
    "    posts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump({'text': posts}, open(f'../Datasets/reddit-jokes-{subreddit}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"jokes\"\n",
    "posts = json.load(open(f'../Datasets/reddit-jokes-{subreddit}.json'))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 1889\n",
      "Average # of characters of submission: 347.065643197459\n",
      "Average # of words of submission: 64.07146638433034\n",
      "Max length of submissions: 9147\n",
      "Min length of submissions: 17\n",
      "Max # of words in submissions: 1648\n",
      "Min # of words in submissions: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Samples: {len(posts)}\")\n",
    "print(f\"Average # of characters of submission: {sum([len(i) for i in posts]) / len(posts)}\")\n",
    "print(f\"Average # of words of submission: {sum([len(i.split()) for i in posts]) / len(posts)}\")\n",
    "print(f\"Max length of submissions: {max([len(i) for i in posts])}\")\n",
    "print(f\"Min length of submissions: {min([len(i) for i in posts])}\")\n",
    "print(f\"Max # of words in submissions: {max([len(i.split()) for i in posts])}\")\n",
    "print(f\"Min # of words in submissions: {min([len(i.split()) for i in posts])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the tokenizer\n",
    "\n",
    "    - <s> is a special token that is meant to fill up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "text = '\\n'.join(posts)\n",
    "open('../Datasets/temp.txt', 'w').write(text)\n",
    "special_tokens = ['<s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(\n",
    "    '../Datasets/temp.txt',\n",
    "    vocab_size=40000,\n",
    "    min_frequency=3,\n",
    "    show_progress=False,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "os.remove('../Datasets/temp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Ä world']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello World\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(f'../Datasets/tokenizer-{subreddit}.json', pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
